## Overview

SignSpeak is a sign language translator developed using Python, OpenCV, NumPy, Keras, Mediapipe, and TensorFlow. This project aims to bridge communication gaps by translating sign language gestures into text or spoken language, making communication more accessible for individuals who are deaf or hard of hearing.

## Features

- Real-time sign language recognition using computer vision techniques.
- Integration of machine learning models for accurate gesture classification.
- Support for translating sign language gestures into text or spoken language.
- Customizable and extendable architecture for adding new gestures and improving translation accuracy.
- Cross-platform compatibility, allowing SignSpeak to run on various devices and operating systems.
  
## Dependencies

- OpenCV: A popular computer vision library used for image and video processing.
- NumPy: A fundamental package for scientific computing with Python, used for array operations and mathematical functions.
- Keras: A high-level neural networks API, used for building and training deep learning models.
- Mediapipe: A library by Google Research for building real-time multi-modal ML pipelines.
- TensorFlow: An open-source machine learning framework developed by Google for training and deploying ML models.
  
## Installation

1. Clone the SignSpeak repository to your local machine.
2. Install the required Python packages listed in the requirements.txt file.
```
pip install -r requirements.txt
```
4. Run the main script to launch the SignSpeak application.
```
python main.py
```

## Usage

1. Position yourself in front of the camera.
2. Perform sign language gestures within the camera's field of view.
3. See the recognized gestures displayed as text or hear them spoken aloud.
4. For more detailed usage instructions and customization options, refer to the documentation provided in the repository.

